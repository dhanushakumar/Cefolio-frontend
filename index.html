import logging
import azure.functions as func
import pandas as pd
import json
from azure.storage.blob import BlobServiceClient

#load Josn Data File from storage Account Via giving container Name.
constr ='DefaultEndpointsProtocol=https;AccountName=seq01655edcdev;AccountKey=B7bmFYbVDgs0Vspm8RmGVp7fjzjbMLp4BhgjP/EVSRuVuIxty3YsCgINy7PteZppR5DFcKbXCawQ7hxMa+y+XQ==;EndpointSuffix=core.windows.net'
blob_service_client = BlobServiceClient.from_connection_string(constr)
container_client = blob_service_client.get_container_client("edam-bizzdesign-poc")
blob_client = container_client.get_blob_client("DataJsonEdamRectified.json")
blob_client.download_blob().readall() # read blob content as string

#Read the Blob downloaded
f = blob_client.download_blob().readall()
data = json.loads(f)

#Reading for all objects in the Data Variable
for i in range(0,len(data)):
  df = pd.DataFrame(data[i])
  normalized1Data = pd.json_normalize(json.loads(df.to_json(orient = 'records')),sep = '_')
  #Not all objects of Data variable have the column Items_documents as the last offset response is Null on items
  if("items_documents" in normalized1Data.columns.values.tolist()):
    normalized1Data = normalized1Data.explode("items_documents")
    #Unwrapping all the nested Json to level 2
    normalized1Data = pd.json_normalize(json.loads(normalized1Data.to_json(orient = 'records')),max_level=2,sep="_")
    #Exploding all the array columns to populate distinct records
    normalized1Data = normalized1Data.explode('items_documents_values_collibra_mastered')
    normalized1Data = normalized1Data.explode('items_documents_values_collibra_origin')
    normalized1Data = normalized1Data.explode('items_documents_values_collibra_provider')
    normalized1Data = normalized1Data.explode('items_documents_values_collibra_secondary')
  #Placing and append the final df to store all the above processed Data.
  if(i==0):
    FinalDf = normalized1Data
  else:
    FinalDf = FinalDf.append(normalized1Data, ignore_index = True)

#Uploading the above Df to conatiner which was mentioned above.
container_client.upload_blob(name='EDaMFlattened.parquet', data=FinalDf.to_parquet())
